{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOlkPJas5pZe",
        "outputId": "afc1afa0-fdcd-484b-c0d2-55f3214bfb79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas-datareader==0.10.0 in /usr/local/lib/python3.10/dist-packages (0.10.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from pandas-datareader==0.10.0) (4.9.4)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.10/dist-packages (from pandas-datareader==0.10.0) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pandas-datareader==0.10.0) (2.31.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23->pandas-datareader==0.10.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23->pandas-datareader==0.10.0) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23->pandas-datareader==0.10.0) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23->pandas-datareader==0.10.0) (1.25.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pandas-datareader==0.10.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pandas-datareader==0.10.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pandas-datareader==0.10.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pandas-datareader==0.10.0) (2024.6.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=0.23->pandas-datareader==0.10.0) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas-datareader==0.10.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install 'ray[tune]'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuS9Osoe5tR2",
        "outputId": "03eeb5a5-05f7-4a46-f865-a506fa4540ee"
      },
      "execution_count": 2,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ray[tune]\n",
            "  Downloading ray-2.30.0-cp310-cp310-manylinux2014_x86_64.whl (66.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.0/66.0 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (8.1.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (3.15.3)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (4.19.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (1.0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (24.1)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (3.20.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (6.0.1)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (2.31.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (2.0.3)\n",
            "Collecting tensorboardX>=1.9 (from ray[tune])\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (14.0.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (2023.6.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow>=6.0.1->ray[tune]) (1.25.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]) (0.18.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[tune]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[tune]) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[tune]) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]) (2024.6.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->ray[tune]) (1.16.0)\n",
            "Installing collected packages: tensorboardX, ray\n",
            "Successfully installed ray-2.30.0 tensorboardX-2.6.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data preprocessing\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import pandas_datareader.data as pdr\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# machine learning\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "\n",
        "# hyperparameter tuning\n",
        "import ray\n",
        "from ray import tune"
      ],
      "metadata": {
        "id": "n8mSQpQf5zOw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "csv_file_path = '/content/merged_tsla_stock_data_with_sentiment.csv'\n",
        "tsla_price_df = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Rename the column 'compound' to 'Sentiment Score'\n",
        "tsla_price_df.rename(columns={'compound': 'Sentiment Score'}, inplace=True)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(tsla_price_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "tW0xR-2E57K5",
        "outputId": "280fc151-4d49-4f1d-c7c5-398e60bdb1d5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/merged_tsla_stock_data_with_sentiment.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-48c16f20764e>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Read the CSV file into a DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcsv_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/merged_tsla_stock_data_with_sentiment.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtsla_price_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Rename the column 'compound' to 'Sentiment Score'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/merged_tsla_stock_data_with_sentiment.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# adding daily returns to stock price data\n",
        "tsla_price_df['Return'] = tsla_price_df['Close'].pct_change()"
      ],
      "metadata": {
        "id": "fQJLdwKU6ExY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting parameters\n",
        "plt.rcParams['figure.figsize'] = (16,9)\n",
        "plt.rcParams['axes.grid'] = True\n",
        "plt.rcParams.update({'font.size': 12})"
      ],
      "metadata": {
        "id": "g9Sgc-co6brd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "start_time_all = datetime.datetime(2019,12,24)\n",
        "end_time_all = datetime.datetime(2022, 3, 1)\n",
        "plt.plot(tsla_price_df.index, tsla_price_df['Close'])\n",
        "print(tsla_price_df['Close'])\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Close Price')\n",
        "plt.xlim(start_time_all, end_time_all)\n",
        "plt.title('TSLA Close Price')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vi0Rab0H6ddz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating correlation between daily returns and sentiment scores\n",
        "tsla_price_df['Return'][1:].corr(tsla_price_df['Sentiment Score'][1:])"
      ],
      "metadata": {
        "id": "FNAcP6XH6fhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting correlation between daily returns and sentiment scores\n",
        "sns.regplot(x=tsla_price_df['Return'][1:], y=tsla_price_df['Sentiment Score'][1:], color='g')\n",
        "plt.title('Correlation between Daily Returns and Sentiment Scores')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0hngxp6y660t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Read the CSV file into a DataFrame, ensuring the first column is read as dates\n",
        "# csv_file_path = 'your_csv_file.csv'\n",
        "# tsla_price_df = pd.read_csv(csv_file_path, parse_dates=[0])\n",
        "\n",
        "# Rename the unnamed column to 'Date'\n",
        "tsla_price_df.rename(columns={tsla_price_df.columns[0]: 'Date'}, inplace=True)\n",
        "\n",
        "# Set the 'Date' column as the index\n",
        "tsla_price_df.set_index('Date', inplace=True)\n",
        "\n",
        "# Drop the 'Return' column if it exists\n",
        "if 'Return' in tsla_price_df.columns:\n",
        "    tsla_price_df.drop(columns=['Return'], inplace=True)\n",
        "\n",
        "# Features (High price, Low price, Open price, Volume, Sentiment Score)\n",
        "X = tsla_price_df.drop(['Close'], axis=1)\n",
        "\n",
        "# Response (Close price)\n",
        "y = tsla_price_df['Close']\n",
        "\n",
        "# Display the features and response DataFrames\n",
        "print(X)\n",
        "print(y)"
      ],
      "metadata": {
        "id": "chu1H3vh68_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_split = 400 # where to split training and validation datasets"
      ],
      "metadata": {
        "id": "omyrCiJe7Lxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape y for scaling\n",
        "y = y.values.reshape(-1, 1)\n",
        "\n",
        "# Normalizing datasets\n",
        "scaler_X = MinMaxScaler()\n",
        "scaler_y = MinMaxScaler()\n",
        "X_norm = scaler_X.fit_transform(X)\n",
        "y_norm = scaler_y.fit_transform(y)\n",
        "\n",
        "# Split index for training and validation\n",
        "n_split = len(tsla_price_df[tsla_price_df.index < '2020-10-01'])\n",
        "\n",
        "# Training using Jan - Sep data\n",
        "# Validating trained model using Oct - Dec data\n",
        "X_train = X_norm[:n_split, :]\n",
        "y_train = y_norm[:n_split, :]\n",
        "\n",
        "X_val = X_norm[n_split:, :]\n",
        "y_val = y_norm[n_split:, :]\n",
        "\n",
        "print('Training Set Shape', X_train.shape, y_train.shape)\n",
        "print('Validation Set Shape', X_val.shape, y_val.shape)"
      ],
      "metadata": {
        "id": "RN9TVydI8YGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting data to tensors\n",
        "X_train_tensors = Variable(torch.Tensor(X_train))\n",
        "y_train_tensors = Variable(torch.Tensor(y_train))\n",
        "\n",
        "X_val_tensors = Variable(torch.Tensor(X_norm))\n",
        "y_true = numpy.array(y)\n",
        "\n",
        "# reshaping X dataset\n",
        "X_train_tensors = torch.reshape(X_train_tensors, (X_train_tensors.shape[0],\n",
        "                                                  1, X_train_tensors.shape[1]))\n",
        "X_val_tensors = torch.reshape(X_val_tensors, (X_val_tensors.shape[0],\n",
        "                                              1, X_val_tensors.shape[1]))\n",
        "\n",
        "print('Training Set Shape   ', X_train_tensors.shape, y_train_tensors.shape)\n",
        "print('Validation Set Shape ', X_val_tensors.shape, y_true.shape)"
      ],
      "metadata": {
        "id": "bzEBpcj58a41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# selecting gpu\n",
        "device = torch.device('cuda')"
      ],
      "metadata": {
        "id": "GNJZ1wh7AOR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM_model(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(LSTM_model, self).__init__()\n",
        "        self.input_size = input_size # number of features\n",
        "        self.hidden_size = hidden_size # number of features in hidden state\n",
        "        self.num_layers = num_layers # number of stacked LSTM layers\n",
        "        self.num_classes = num_classes # number of output classes\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
        "                            num_layers=num_layers, batch_first=True) # LSTM layer\n",
        "        self.fc = nn.Linear(hidden_size, num_classes) # fully connected last layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = Variable(torch.zeros(self.num_layers, x.size(0),\n",
        "                                  self.hidden_size)).to(device) # hidden state\n",
        "        c0 = Variable(torch.zeros(self.num_layers, x.size(0),\n",
        "                                  self.hidden_size)).to(device) # internal state\n",
        "        # propagating input through LSTM\n",
        "        out, _ = self.lstm(x, (h0, c0)) # out: tensor of shape (seq_length, batch_size, hidden_size)\n",
        "        # decoding hidden state of last time step\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ],
      "metadata": {
        "id": "4M3rQvavAQbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# starting ray runtime\n",
        "ray.init()"
      ],
      "metadata": {
        "id": "11PK4vavASSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking available resources\n",
        "ray.cluster_resources()"
      ],
      "metadata": {
        "id": "5uV7JYyHAWeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function for calculating MSE (Mean Squared Error)\n",
        "def calc_mse(y_true, y_hat):\n",
        "    mse = float(sum((y_true - y_hat) * (y_true - y_hat)) / len(y_hat))\n",
        "    return mse"
      ],
      "metadata": {
        "id": "hRXtFtjsAZxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function for fitting and evaluating model using different hyperparameters\n",
        "def model_fit_eval(config=None, best_config=None, save_model_state=False):\n",
        "    if best_config is None:\n",
        "        hidden_size = config['hidden_size']\n",
        "        num_layers = config['num_layers']\n",
        "        learning_rate = config['learning_rate']\n",
        "        # learning rate controls how much to change model in response to estm error each time model weights are updated\n",
        "        num_epochs = config['num_epochs']\n",
        "    else:\n",
        "        hidden_size = best_config['hidden_size']\n",
        "        num_layers = best_config['num_layers']\n",
        "        learning_rate = best_config['learning_rate']\n",
        "        num_epochs = best_config['num_epochs']\n",
        "\n",
        "    input_size = 6\n",
        "    num_classes = 1\n",
        "\n",
        "    model = LSTM_model(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) # adam optimizer\n",
        "    # algorithms/methods to change attributes of neural network such as weights and learning rate to reduce losses\n",
        "\n",
        "    loss_function = torch.nn.MSELoss() # mean-squared error of regression\n",
        "    # loss function measures how bad model performs: high loss -> low accuracy\n",
        "\n",
        "    for epoch in range(num_epochs + 1):\n",
        "        outputs = model.forward(X_train_tensors.to(device)) # forward pass\n",
        "        optimizer.zero_grad() # calculating gradient, manually setting to 0\n",
        "        loss = loss_function(outputs, y_train_tensors.to(device)) # obtaining loss\n",
        "        loss.backward() # calculating loss of loss function\n",
        "        optimizer.step() # improving from loss, i.e. backprop\n",
        "        if best_config is not None:\n",
        "            if epoch % 1000 == 0:\n",
        "                print(f'Epoch: {epoch}, Loss: {loss.item():.4f}')\n",
        "\n",
        "    # saving model state\n",
        "    if save_model_state:\n",
        "        torch.save(model.state_dict(), '4-model-state.pth')\n",
        "\n",
        "    # loading model state\n",
        "    # model = LSTM_model(*args, **kwargs)\n",
        "    # model.load_state_dict(torch.load(PATH))\n",
        "    # model.eval()\n",
        "\n",
        "    # predicting response from model\n",
        "    y_hat = model(X_val_tensors.to(device)) # forward pass\n",
        "    y_hat = y_hat.data.detach().cpu().numpy() # numpy conversion\n",
        "    y_hat = scaler.inverse_transform(y_hat) # inverse transformation\n",
        "\n",
        "    if best_config is None:\n",
        "        mse = calc_mse(y_true, y_hat)\n",
        "        tune.report(mse=mse)\n",
        "    else:\n",
        "        return y_hat"
      ],
      "metadata": {
        "id": "2mBEIC-dAbrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# USE SSA HERE!!\n",
        "\n",
        "# # using grid search to get best hyperparameters\n",
        "# analysis = tune.run(\n",
        "#     model_fit_eval,\n",
        "#     config={'hidden_size': tune.grid_search([2, 3, 5]),\n",
        "#             'learning_rate': tune.grid_search([0.0005, 0.001, 0.002]),\n",
        "#             'num_epochs': tune.grid_search([2000, 4000, 8000]),\n",
        "#             'num_layers': tune.grid_search([1, 2, 4])},\n",
        "#     resources_per_trial={'cpu': 2, 'gpu': 1}) # leveraging all resources\n",
        "\n",
        "#   # visualizing hyperparameter tuning results\n",
        "# %load_ext tensorboard\n",
        "# %tensorboard --logdir ~/ray_results\n",
        "\n",
        "# # best hyperparameters\n",
        "# best_trial = analysis.get_best_trial(metric='mse', mode='min', scope='all')\n",
        "# min_mse = best_trial.metric_analysis['mse']['avg']\n",
        "# print(f'Min MSE: {min_mse}')\n",
        "# best_trial_config = best_trial.config\n",
        "# print(f'Best trial config: {best_trial_config}')\n",
        "\n",
        "# # training model using best hyperparameters\n",
        "# best_trial_config['num_epochs'] = 10000 # increasing num of epochs\n",
        "# y_hat = model_fit_eval(best_config=best_trial_config, save_model_state=True)\n",
        "\n",
        "# # shutting down ray runtime\n",
        "# ray.shutdown()\n"
      ],
      "metadata": {
        "id": "kvBWOdmbAenq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function for plotting actual and predicted Adj Close price\n",
        "y_hat=[]\n",
        "def plt_graph(start_time, end_time, period, val_line=False):\n",
        "    # plotting actual Adj Close price\n",
        "    plt.plot(tsla_price_df.index, y_true, label='Actual Price')\n",
        "    # plotting predicted Adj Close price\n",
        "    plt.plot(tsla_price_df.index, y_hat, label='Pred Price')\n",
        "    if val_line:\n",
        "        plt.axvline(x=start_time_val, c='r', linestyle='--')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Adj Close Price')\n",
        "    plt.xlim(start_time, end_time)\n",
        "    plt.title(f'{period} Period')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "SzhZIPIYAlNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # plotting all period\n",
        "# period = 'All'\n",
        "# plt_graph(start_time_all, end_time_all, period, True)"
      ],
      "metadata": {
        "id": "CQI7Ke_KCzSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_mse(y_true, y_hat):\n",
        "    if len(y_true) != len(y_hat):\n",
        "        raise ValueError(\"Length of y_true and y_hat must be the same.\")\n",
        "\n",
        "    mse = np.mean((y_true - y_hat) ** 2)\n",
        "    return mse"
      ],
      "metadata": {
        "id": "Bg3hhJT1AyEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mse = calc_mse(y_true, y_hat)                     ### AFTER IMPLEMENTING SSA\n",
        "# print(f'MSE ({period}): {mse}')"
      ],
      "metadata": {
        "id": "9sBToZAsCfeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # plotting training period\n",
        "# period = 'Training'\n",
        "# plt_graph(start_time_all, end_time_train, period)"
      ],
      "metadata": {
        "id": "6W2_UwMfCmCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mse = calc_mse(y_true[:n_split], y_hat[:n_split])\n",
        "# print(f'MSE ({period}): {mse}')"
      ],
      "metadata": {
        "id": "vlZsbmRYCofo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # plotting validation period\n",
        "# period = 'Validation'\n",
        "# plt_graph(start_time_val, end_time_all, period)"
      ],
      "metadata": {
        "id": "KexDy0rECqhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mse = calc_mse(y_true[n_split:], y_hat[n_split:])\n",
        "# print(f'MSE ({period}): {mse}')"
      ],
      "metadata": {
        "id": "ow5BOdLhCsiM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}